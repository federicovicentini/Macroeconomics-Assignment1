demo()
demo(graphics.)ù
demo(graphics)
demo
demo()
demo(graphics)
plot(1,1)
x<-(0,1,2,3,4)
x<-3
x
data()
plot(quakes)
plot(morley)
plot(longley)
library(foreign)
library(sandwich)
library(lmtest)
library(AER)
library(stargazer)
library(tseries)
library(dynlm)
library(forecast)
library(quantmod)
library(tsapp)
tilde <- rawToChar(as.raw(126)) #Command needed to use the Tilde character in the coding
EU=read.csv("Categorical_EPU_Data-ritaglio.csv",sep=";")
EU[,1]=as.Date.character(EU[,1], "%d/%m/%Y")
names(EU)[1]="DATE"
names(EU)[2]="EPU"
names(EU)[3]="MP"
names(EU)[4]="FP"
names(EU)[5]="TAX"
names(EU)[6]="GOV"
names(EU)[7]="HEALTH"
names(EU)[8]="DEFENSE"
names(EU)[9]="ENTITLEMENT"
names(EU)[10]="REG"
names(EU)[11]="FINREG"
names(EU)[12]="TRADE"
names(EU)[13]="DEBTCURRENCY"
unemployment=read.csv("UNRATE-tagliato.csv")
EU[14]=unemployment$UNRATE
names(EU)[14]="UNEMPLOYMENT"
proxy=read.csv("instruments_web-ritaglio.csv",sep=";")
names(proxy)[1]="DATE"
names(proxy)[2]="IS-FOMC"
names(proxy)[3]="IV4"
names(proxy)[4]="IV5"
EU[15]=proxy$IV4
names(EU)[15]="PROXY"
#First Regression
regressionbase=dynlm(EPU~Lag(EPU,1)+UNEMPLOYMENT+PROXY,data=EU)
HAC=coeftest(regressionbase, vcov. = vcovHAC(regressionbase)) #This is the code for HAC robust standard errors
ses=list(HAC[,2])
ps=list(HAC[,4])
#RIDGE REGRESSION
library(ISLR)
head(Hitters)
complete.cases(Hitters)
rows_to_sel=complete.cases(Hitters)
hit=Hitters[rows_to_sel,]
complete.cases(hit)
#hit contains only the complete records (rows), with no missing values
head(hit)
#Note that there are cathegorical vars; we need to transform them into dummies
X=model.matrix(Salary ~ .,data=hit)[,-c(1,2)]
#used to convert cathegorical vars into dummy vars
#Responde is salary, x is all the other variables
y=hit$Salary
#Ridge and Lasso are implemented in library glmnet
library(glmnet)
#With alpha = 1, Lasso penalty is in place, with alplha=0, we have Ridge regression
ridge_0=glmnet(X,y,alpha=0, lambda=0)
summary(ridge_0)
#we want to run ridge with lambda>0, so we use a grid of values for lambda
grid=10^seq(-2,10, length=100)
grid
plot(grid)
ridge_grid=glmnet(X,y,alpha=0, lambda=grid, standardize=TRUE)
#I implement ridge 100 times, each with a different value of lambda in grid
#See the coefficient estimates
coef_grid=coef(ridge_grid)
#I want to select the 100 cpefficients associated with predictor hits (the second row)
coef_grid[2,] #these is the collection of estimates for beta_1 as a function of lambda
#Pay attention! results are sorted by decreasing values of lambda. Let me sort for increasing val of lambda
100:1
beta_1_ridge=coef_grid[2,100:1]#Now sorted by increasing values of lambda
plot(beta_1_ridge,type="l", xlab=expression(lambda), ylab=expression[beta])
plot(beta_1_ridge,type="l", xlab=expression(lambda), ylab=expression[beta[2]])
beta_2_ridge=coef_grid[3,100:1]
source('C:/Users/Documenti Fede/Scuola/Università/MSc in Economics/Statistical Modelling/R Files/lesson7.R', echo=TRUE)
plot(beta_1_ridge,type="l", xlab=expression(lambda))
plot(beta_2_ridge,type="l", xlab=expression(lambda))
cv.glmnet(X,y)
set.seed(1)
cv.glmnet(X,y)
set.seed(1)
cv.glmnet(X,y, lambda=grid)
set.seed(1)
cv.glmnet(X,y, lambda=grid, alpha=0)
cv_out=cv.glmnet(X,y, lambda=grid, alpha=0)
plot(cv_out)
#RIDGE REGRESSION
library(ISLR)
head(Hitters)
complete.cases(Hitters)
rows_to_sel=complete.cases(Hitters)
hit=Hitters[rows_to_sel,]
complete.cases(hit)
#hit contains only the complete records (rows), with no missing values
head(hit)
#Note that there are cathegorical vars; we need to transform them into dummies
X=model.matrix(Salary ~ .,data=hit)[,-c(1,2)]
#used to convert cathegorical vars into dummy vars
#Responde is salary, x is all the other variables
y=hit$Salary
#Ridge and Lasso are implemented in library glmnet
library(glmnet)
#With alpha = 1, Lasso penalty is in place, with alplha=0, we have Ridge regression
ridge_0=glmnet(X,y,alpha=0, lambda=0)
summary(ridge_0)
#we want to run ridge with lambda>0, so we use a grid of values for lambda
grid=10^seq(-2,10, length=100)
grid
plot(grid)
ridge_grid=glmnet(X,y,alpha=0, lambda=grid, standardize=TRUE)
#I implement ridge 100 times, each with a different value of lambda in grid
#See the coefficient estimates
coef_grid=coef(ridge_grid)
#I want to select the 100 cpefficients associated with predictor hits (the second row)
coef_grid[2,] #these is the collection of estimates for beta_1 as a function of lambda
#Pay attention! results are sorted by decreasing values of lambda. Let me sort for increasing val of lambda
100:1
beta_1_ridge=coef_grid[2,100:1]#Now sorted by increasing values of lambda
beta_2_ridge=coef_grid[3,100:1]
plot(beta_1_ridge,type="l", xlab=expression(lambda))
plot(beta_2_ridge,type="l", xlab=expression(lambda))
#How to choose lmbda?
#Use CV methods, specifically, k-fold cross validation to choose among models
#This is implemented in function cv.glmnet
set.seed(1)
cv_out=cv.glmnet(X,y, lambda=grid, alpha=0)
plot(cv_out)
cv.out=cv.glmnet(X,y, lambda=grid, alpha=0)
plot(cv.out)
cv.out$lambda
cv.out$lambda.min
set.seed(1)
cv.out=cv.glmnet(X,y, lambda=grid, alpha=0)
plot(cv.out)
cv.out$lambda
cv.out$lambda.min
set.seed(1)
cv.out=cv.glmnet(X,y, lambda=grid, alpha=0)
plot(cv.out)
cv.out$lambda
lambda_hat=cv.out$lambda.min
cv.out.opt=glmnet(X,y,lambda_hat, alpha=0) #alpha=0 is for ridge regression
round(cv.out.opt$beta,2)
cv.out.opt=glmnet(X,y,lambda=lambda_hat, alpha=0) #alpha=0 is for ridge regression
round(cv.out.opt$beta,2)
grid
lasso_grid=glmnet(X,y, lambda=grid, alpha=1)
coef(lasso_grid)
grid
lasso_grid=glmnet(X,y, lambda=grid, alpha=1)
coef_grid=coef(lasso_grid)
beta_1_lasso=coef_grid[2,100:1]
plot(beta_1_lasso, type="l")
plot(beta_1_lasso, type="l", col="red")
beta_1_lasso=coef_grid[2,100:1]
beta_2_lasso=coef_grid[3,100:1]
plot(beta_1_lasso, type="l", col="red")
plot(beta_2_lasso, type="l", col="blue")
lasso_grid=cv.glmnet(X,y,lambda=grid,alpha=1)
plot(lasso_grid)
lambda_hat=lasso_grid$lambda.min
lambda_hat
lasso_hat=glmnet(X,y,lambda = lambda_hat, alpha=1)
lasso_hat$beta
knitr::opts_chunk$set(echo = TRUE)
# Clear the variables
rm(list = ls())
# Set the working directory to source file location with
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Install packages
packages <- c("tidyverse", "rsdmx", "eurostat", "tbl2xts",
"tidyquant", "BCDating", "pwt10", "dplyr",
"stargazer", "car")
knitr::opts_chunk$set(echo = TRUE)
# Set the working directory to source file location with
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
if (length(new.packages)) install.packages(new.packages)
invisible(lapply(packages, library, character.only = TRUE))
## POINT 1
First of all, we need to download the USA data from the Fred database, using a for cycle and the function $\texttt{getSymbols()}$
nipa <- c("EXPGS", "IMPGS", "PCEC", "GDP", "GPDI", "GCE")
for (i in 1:length(nipa)) {
getSymbols(nipa[i], src = "FRED")
}
# Federico Vicentini
# 23/05/2022
# Macroeconomics - Assignment no.1
#################
##### POINT 1#####
#################
# Clear the variables
rm(list = ls())
# Set the working directory to source file location with
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Load packages
library(quantmod)
library(eurostat)
# Get Data from Fred website
nipa <- c("EXPGS", "IMPGS", "PCEC", "GDP", "GPDI", "GCE")
for (i in 1:length(nipa)) {
getSymbols(nipa[i], src = "FRED")
}
# Plot residuals from NIPA equation
((EXPGS - IMPGS + PCEC + GPDI + GCE) - GDP)^2
plot(((EXPGS - IMPGS + PCEC + GPDI + GCE) - GDP)^2)
# GET THE OTHER TWO COUNTRIES FROM EUROSTAT DATABASE
# Install packages
packages <- c("tidyverse", "rsdmx", "eurostat", "tbl2xts",
"tidyquant", "BCDating", "pwt10", "dplyr")
new.packages <- packages[!(packages %in% installed.packages()[, "Package"])]
if (length(new.packages)) install.packages(new.packages)
invisible(lapply(packages, library, character.only = TRUE))
# Get Eurostat data listing
toc <- get_eurostat_toc()
# Check the first items
library(knitr)
kable(tail(toc))
library(xts)
library(ecb)
# For the original data, see
# http://ec.europa.eu/eurostat/tgm/table.do?tab=table&init=1&plugin=1&language=en&pcode=tsdtr210
# GDP download
namq_10_gdp <- get_eurostat("namq_10_gdp",
stringsAsFactors = FALSE
)
# Create vector with list of countries
nimacountries <- c("ES", "FR")
# Create a vector with codes for NIMA aggregates
nimaeu <- c("B1GQ", "P31_S14_S15", "P3_S13", "P51G", "P52", "P53", "P6", "P7")
# Create a vector with names for columns in matrix dataeu
colname <- c(
"GDP", "C", "G", "I", "inv", "saldo", "X", "IM",
"GDPb", "Cb", "Gb", "Ib", "invb", "saldob", "Xb", "IMb"
)
# Create matrix dataeu to fill with data from the 2 countries
dataeu <- matrix(NA, 89, length(nimacountries) * length(nimaeu))
colnames(dataeu) <- colname
# Set z to partition columns
z <- length(colname) / length(nimacountries)
# For cycle to fill dataeu with data from the countries
for (i in 1:length(nimacountries)) {
for (s in 1:length(nimaeu)) {
newdata <- namq_10_gdp %>%
filter(geo == nimacountries[i]) %>%
filter(time >= "2000-01-01") %>%
filter(unit == "CP_MEUR") %>%
filter(s_adj == "SCA") %>%
filter(na_item == nimaeu[s])
newdata$time <- convert_dates(newdata$time)
newdataxts <- xts(newdata$values, newdata$time)
dataeu[, (s + (z * (i - 1)))] <- newdataxts
}
}
# Change Imports from positive to negative in both countries
dataeu[, z] <- dataeu[, z] * (-1)
dataeu[, length(colname)] <- dataeu[, length(colname)] * (-1)
# Convert dataeu to xts format
dataeu <- xts(dataeu, rev(newdata$time))
# Check NIMA identity for country 1
#Create empty vectors for sums of aggregates and for gdp
sums <- c()
gdp <- c()
#Fill them with the data using a for cycle
for (i in 1:nrow(dataeu)) {
sums[i] <- sum(dataeu[i, -c(1, (z + 1):length(colname))])
gdp[i] <- dataeu[i, 1]
}
#Plot the values
sum(dataeu[i, -c(1, z:length(colname))]) - dataeu[i, z]
dataeu[, -c(1, z:length(colname))]
#Plot the graphs
plot(sums, col = "green", type = "l")
lines(gdp, col = "red", type = "l")
plot(sums - gdp)
# Check NIMA identity for country 2
# Code is the same as before
sums2 <- c()
gdp2 <- c()
for (i in 1:nrow(dataeu)) {
sums2[i] <- sum(dataeu[i, -c(1:(z + 1))])
gdp2[i] <- dataeu[i, (z + 1)]
}
sum(dataeu[1, -c(1:(z + 1))]) - dataeu[1, length(colname)]
dataeu[i, (z + 1)]
dataeu[1, -c(1:(z + 1), length(colname))]
plot(sums2, col = "green", type = "l")
lines(gdp2, col = "red", type = "l")
plot(sums2 - gdp2)
#################
##### POINT 2####
#################
#Import the library
library(BCDating)
# Generate a matrix to fill with the log of the variables
data_bc <- matrix(NA, length(GDP), 4)
# Yaking logs
data_bc[, 1] <- log(GDP)
data_bc[, 2] <- log(IMPGS)
data_bc[, 3] <- log(GPDI)
data_bc[, 4] <- log(PCEC)
for (count in 1:4) {
# create a time series with the variable[count]
# setting the time index as year-quarter
data_ts <- ts(data_bc[, count], start = c(1947, 1), frequency = 4)
# applied the BBQ method
bc_US <- BBQ(data_ts, name = count)
# plot the results
summary(bc_US)
plot(bc_US, data_ts)
}
#################
##### POINT 3#####
#################
#Load the necessary libraries
library(pwt10)
library(dplyr)
#Download penn database from the web
data("pwt10.0")
penn <- pwt10.0
rm(pwt10.0)
#List of the years we are interested in (they are 10 years apart)
list <- c("1950", "1960", "1970", "1980", "1990", "2000", "2010")
#Filter Penn data for the USA
uspenn <- penn %>%
filter(country == "United States of America")
#Select only labour share of total income and the year
uslabsh <- uspenn %>%
select(c(year, labsh))
#Code is the same for the other 2 countries
espenn <- penn %>%
filter(country == "Spain")
eslabsh <- espenn %>%
select(c(year, labsh))
frpenn <- penn %>%
filter(country == "France")
frlabsh <- frpenn %>%
select(c(year, labsh))
#Filter each time series only for the years we selected
#then repeat for each country
usfilt <- uslabsh %>%
filter(year %in% list)
plot(usfilt$year, usfilt$labsh,
ylim = c(0, 1),
type = "b",
ylab = "Labour Share of Total Income",
xlab = "Year", main = "US"
)
esfilt <- eslabsh %>%
filter(year %in% list)
plot(usfilt$year, esfilt$labsh,
ylim = c(0, 1),
type = "b",
ylab = "Labour Share of Total Income",
xlab = "Year", main = "SPAIN"
)
frfilt <- frlabsh %>%
filter(year %in% list)
plot(frfilt$year, frfilt$labsh,
ylim = c(0, 1),
type = "b",
ylab = "Labour Share of Total Income",
xlab = "Year", main = "FRANCE"
)
#Create the list of countries we will use
countries <- c(
"United States of America", "France", "Germany", "Japan",
"Canada", "China",
"Italy", "Netherlands", "United Kingdom", "Spain"
)
char <- c("country", "rgdpna", "rtfpna", "rnna", "avh", "emp")
#Select years between 1960 and 2000
penn_years <- penn %>%
filter((year >= 1960) & (year <= 2000) & (country %in% countries))
penn_years <- select(penn_years, char)
#Create function ration to...
ratio <- function(x) {
x <- log(x)
out <- c(x[1])
for (count in 2:length(x)) {
out <- append(out, (x[count] - x[count - 1]) / x[count - 1])
}
return(out)
}
cap <- function(x) {
L <- x[, 2] * x[, 3]
k <- x[, 1] / L
alpha <- 0.3
out <- ratio(k) * alpha
return(out)
}
penn_years$gdp_ratio <- ratio(penn_years[, 2])
penn_years$tfp_ratio <- ratio(penn_years[, 3])
penn_years$cap_deepe <- cap(penn_years[, c("rnna", "avh", "emp")])
penn_years$tfp_share <- penn_years$tfp_ratio / penn_years$gdp_ratio
penn_years$cap_share <- penn_years$cap_deepe / penn_years$gdp_ratio
View(penn_years[, c("country", "gdp_ratio", "tfp_ratio",
"cap_deepe", "tfp_share", "cap_share")])
#################
##### POINT 4#####
#################
#Download database from Favero's book
favero <- read.csv("MRW.csv", sep = ";", dec = ",")
#Delete the ID column (it is useless for our purposes)
favero$ID <- NULL
#Create a dataframe with only the variables we are interested in
replica <- data.frame(favero$YL85, favero$N6085, favero$IY)
names(replica) <- c("Ypc", "PopGrowth", "IonY")
#Log-transform our variable of interest and add g+delta to n
replica$Ypc <- log(replica$Ypc)
replica$PopGrowth <- log(((replica$PopGrowth) / 100) + 0.05)
replica$IonY <- log((replica$IonY) / 100)
names(replica)[2] <- "ngdelta"
#Replicate regression results
reg <- lm(Ypc ~ IonY + ngdelta, data = replica)
#Print them using stargazer function
library(stargazer)
stargazer(reg, type = "text")
#Do hypothesis testing with the two provided restrictions
#(which were respectively b1=-b2 ; b1=0.5 & b2=-0.5)
library(car)
linearHypothesis(reg, c("IonY-ngdelta=0"), test = "F")
linearHypothesis(reg, c("IonY=0.5", "ngdelta=-0.5"), test = "F")
#Perform bootstrapping using Boot from the package car
Boot(reg)
#Load library boot
library(boot)
#Create bs function to get coefficients from the regression
bs = function(formula, data, indices){
d = data[indices,]
fit=lm(formula, data=d)
return(coef(fit))
}
#Create var results with function boot on the regression
results = boot(replica, statistic=bs, R=1000, formula= Ypc ~ IonY + ngdelta)
#Print the results
results
#Add schooling the replica database
replica$school=log((favero$SCHOOL)/100)
#Replicate the augmented Solow model
reg2 = lm(Ypc ~ IonY + ngdelta + school, data = replica)
#Print results using the Stargazer function
stargazer(reg2, type = "text")
View(namq_10_gdp)
str(namq_10_gdp$time)
View(dataeu)
View(data_bc)
View(data_ts)
data_ts
View(favero)
